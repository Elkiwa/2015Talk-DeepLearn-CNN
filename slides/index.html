<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CNN From the Ground Up, Liang2</title>
  <meta name="viewport" content="width=792, user-scalable=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <!-- Icon -->
  <link href="pics/favicon.png" rel="icon" type="image/x-icon" />
  <!-- MathJax -->
  <!-- CSS Stle -->
  <link rel="stylesheet" href="lib/shower/themes/ribbon/styles/screen.css">
  <link rel="stylesheet" href="lib/highlight/styles/tomorrow.css" type="text/css"/>
  <link rel="stylesheet" href="static/custom.css" type="text/css"/>
</head>
<body class="list">
  <!-- Header in overview -->
  <header class="caption">
    <h1>Convolutional Neural Network From the Ground Up</h1>
    <p style="line-height: 32px; padding-top:15px;"><a href="http://liang2.tw">Liang Bo Wang (亮亮)</a>, 2015-05-16</p>
  </header>

  <!-- START OF SLIDE CONTENT -->

  <!-- Cover slide -->
  <section id="cover" class="slide cover w"><div>
    <h3 id="talk-subheader">Taiwan R User Group, 2015-05-18</h3>
    <h2 id="talk-header" class="place">Convolutional Neural Network (CNN) From the Ground Up</h2>
    <p id="talk-orig-author">
      Orig. <a href="http://cs231n.stanford.edu/">Stanford CS231n course</a> by Andrej Karpathy and <br> Li Fei-Fei under MIT License
    </p>
    <p id="talk-author">
      Adapted by <a href="http://liang2.tw" target="_blank">Liang<sup>2</sup></a> under CC 4.0 BY license
    </p>
    <p id="usage-instr">
      <kbd>Esc</kbd> to overview <br />
      <kbd>←</kbd> <kbd>→</kbd> to navigate
    </p>
    <img src="pics/cover.jpg" alt="">
  </div>
  <style>
    #talk-header {
      color: #D91A3C;
      text-align: center;
      font-size: 80px;
      line-height: 1.2em;
      opacity: 0.8;
      position: relative;
      top: 20px;
      width: 120%;
    }
    #talk-subheader {
      color: #0376A6;
      text-align: center;
      font-size: 32px;
      opacity: 0.3;
      position: relative;
      top: -90px;
    }
    #talk-orig-author,
    #talk-author {
      position: relative;
      font-size: 24px;
      line-height: 1.2em;
      text-shadow: 1px 1px 3px #000;
    }
    #talk-orig-author {
      top: -60px;
    }
    #talk-author {
      top: -50px;
    }
    #talk-orig-author a,
    #talk-author a {
      color: #FFFA20;
    }
    #cover p {
      margin: 10px 0 0;
      text-align: center;
      color: #FFF;
      font-size: 32px;
      opacity: 0.8;
    }
    #usage-instr {
      position: absolute;
      text-align: right;
      right: 30px;
      bottom: 20px;
    }
    #usage-instr kbd {
      opacity: 0.8;
      color: #D91A3C;
      background-color: white;
    }
    #cover .src-link {
      position: absolute;
      font-size: 14px;
      text-align: right;
      bottom: 10px;
      right: 10px;
    }
    #cover img {
      opacity: 1;
    }
  </style>
  </section>

  <section id="disclaimer" class="slide w shout"><div>
      <h2>I don't own / create most of the content</h2>
    </div>
    <style>
      #disclaimer h2 {
        color: #D91A3C;
      }
    </style>
  </section>

  <section id="fast-pace" class="slide w shout"><div>
      <h2>Far more details about DNN beside this talk</h2>
    </div>
    <style>
      #fast-pace h2 {
        color: #0376A6;
      }
    </style>
  </section>

  <section id="about-me" class="slide"><div>
      <h2>About Me</h2>
      <ul>
        <li>Master student at <br />
          Bioinfo &amp; Biostat Core Lab, NTU CGM</li>
        <li>R / Python</li>
        <li>Taiwan R co-organizer</li>
        <li>PyCon APAC 2014-15/TW staff</li>
        <li>Former intern at Microsoft Research Asia</li>
        <li>Happy to chat about DNA and anime</li>
      </ul>
      <img id="protrait" src="pics/me.jpg" class="place r"alt="">
    </div>
    <style>
      #protrait {
        margin-right: 120px;
        width: 300px;
      }
    </style>
  </section>

  <section id="super-img-class" class="slide twocol"><div>
      <h2>Supervised Image Classification</h2>
      <div class="left">
        <ul>
          <li><strong>Input:</strong> RGB pixel values</li>
          <li><strong>Output:</strong> classes / labels, usually with a score</li>
          <li>The classifier learns their relationship from data</li>
          <li>So data are splitted into training / validation / testing datset</li>
        </ul>
      </div>
      <div id="cite-lovelive" class="place b r citation">
        <img src="pics/external/lovelive_classification.jpg" alt="">
        <p>Ref: <a href="http://christina.hatenablog.com/entry/2015/01/23/212541">Deep Learningでラブライブ！キャラを識別する</a></p>
      </div>
      <style>
        #super-img-class .left {
          width: 40%;
        }
        #cite-lovelive {
          margin-bottom: 50px;
          width: 420px;
        }
      </style>
  </div></section>

  <section id="end-to-end" class="slide"><div>
      <h2>CNN / DL are end-to-end classification</h2>
      <div class="citation">
        <img src="pics/external/lenet.png" alt="">
        <p>Ref: <a href="http://deeplearning.net/tutorial/lenet.html">Deeplearning.net LeNet tutorial</a></p>
      </div>
      <ul>
        <li>The CNN idea exists for 15+ years [LeCun 1998]</li>
        <li>A feeding frenzy in recent 5 fives thanks to tons of news coverage and GPU computation</li>
        <li>We will implement it with our bare hands today (though skipping <em>a lot</em>)</li>
      </ul>
  </div></section>

  <section id="general-ml-framework" class="slide"><div>
      <h2>General machine learning framework</h2>
      <ol>
        <li>A mapping function \(f\) from input features \(x\) to scores of output class \(f\).
          Class with highest score is chosed as the prediction for input \(i\).
          $$
          \hat{y_i} = \arg\max_j f(x_i; W)_j \\
          j, y_i \in \{1 \ldots K\}
          $$</li>
        <li>A loss function \(L\) to compute the difference between \(y_i\) and \(\hat{y_i}\).</li>
        <li>Optimize (Lower) the loss by updating the parameter \(W\).</li>
        <li>Stop the update when max iterations are reached or stop criteria meets.</li>
      </ol>
  </div></section>

  <section id="linear-class-cover" class="slide cover w subheader" ><div>
      <h2>Softmax Linear Classification</h2>
      <img src="pics/mid.jpg" alt="">
    </div>
    <style>
      #linear-class-cover h2 {
      }
    </style>
  </section>

  <section class="slide"><div>
      <h2>Linear classifier</h2>
      <p>Training dataset of images \(x_i \in R^D\) each with label \(y_i\) for \(N\) examples, \(i = 1 \ldots N\) and \(y_i \in 1 \ldots K\).
        $$
        f(x_i; W, b) = W x_i + b \hspace{2em} f: R^D \mapsto R^K
        $$
        where parameters are \(W\) [K x D] called <strong>weights</strong> and \(b\) [K x 1] called <strong>bias vector</strong>.
       </p>
      <p>In CIFAR-10 dataset, \(N\) = 50,000, \(D\) = 32 x 32 x 3 = 3072, \(K\) = 10.</p>
  </div></section>

  <section class="slide"><div>
      <h2>Linear classifier visualized</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_linear_classification.png" alt="">
        <p>Ref: <a href="http://cs231n.github.io/linear-classify/">CS231n Note: Linear Clasification</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Bias trick</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_bias_trick.png" alt="">
        <p>Ref: <a href="http://cs231n.github.io/linear-classify/">CS231n Note: Loss optimization</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Loss function - Softmax</h2>
      <p>
        $$
        L_i = -\log\left(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}\right)
        $$
        \(e^{f_{y_i}}\) is like probability yet being normalized. Ex. \(-\log 10 &lt; -\log 0.01 \). Higher score leads to lower loss. <br>
        For numerical stability, in computation \(
        \frac{e^{f_{y_i}}}{\sum_j e^{f_j}}
        = \frac{C \cdot e^{f_{y_i}}}{C \cdot \sum_j e^{f_j}}
        = \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}
        \) and let \(
          \log C = -\max_j f_j
        \) to normalized the exp. of the largest score to be 1.</p>
  </div></section>

  <section class="slide"><div>
      <h2>L2 Regularization</h2>
      <ul>
        <li>Set of parameters \(W\) having the lowest loss is not unique. \(\phi W, \phi &gt; 1\)</li>
        <li>Regularization of \(L2\) norm and a hyperparameter \(\lambda\) to tune the strength
          $$
            R(W) = \sum_k\sum_l W_{k,l}^2
          $$
        </li>
        <li>Full loss function includes avg. data loss and regularization loss
          $$
          L =
            \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} +
            \underbrace{ \lambda R(W) }_\text{regularization loss}
          $$
        </li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Regularization (cont'd)</h2>
      <ul>
        <li>Penalizing large weights tends to improve generalization.</li>
        <li>For example, $$ x = [1, 1, 1, 1] \hspace{1em} w_1 = [1, 0, 0, 0] \hspace{1em} w_2 = [0.5, 0.5, 0.5, 0.5]$$
        where their L2 penalty are 0.5 and 0.25 respectively, \(w_2\) is preferred.</li>
        <li>L2 penalty prefers smaller and more diffuse weight vectors. Imply using more input dimensions instead of betting on one or two.</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Loss optimization</h2>
      <ul>
        <li>How to update the parameters based on this loss function?</li>
        <li>It's like being lost in a mountain and try to find a way home, no map.</li>
        <li>Height analogs to our loss function; position is our 2D parameters.</li>
        <li>Strategy: <strong>gradients of the loss function</strong>. <br>
        Feel feet on your ground, follow the slope of hills.
          $$
            \frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
          $$
        </li>
        <li>Called numerical gradients. Practically use \([f(x + h) -  f(x - h)] / 2h\)</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Choose the step size (learning rate)</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_vis_loss_fun.png" alt="">
        <p>Ref: <a href="http://cs231n.github.io/optimization-1/">CS231n Note: Linear Clasification</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Analytical gradients</h2>
      <ul>
        <li>Numerical gradients is slow (think of computing this for all \(W\) dimensions.</li>
        <li>We know Calculus ... so what's the gradient of softmax?
          $$
          \frac{\partial L_i}{ \partial f_j} = - \left[\mathbb{1}(i = j) - p_j\right] \hspace{1em}
            p_i = \frac{e^{f_i}}{\sum e^{f}} \hspace{1em}
            \frac{\partial f_j}{ \partial w_j} = x_j
          $$
        </li>
        <li>Use the chain rule to get the gradient for
          \(\frac{\partial L_i}{ \partial w_j}
          = \frac{\partial L_i}{ \partial f_j} \cdot \frac{\partial f_j}{ \partial w_j}\)
        </li>
        <li>Already on edge? Check with the numerical gradients.</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Gradient Descent</h2>
      <pre class="language-python tighter">
      <code># Vanilla Minibatch Gradient Descent
while it &lt; max_iter:
    data_batch = sampling(data, 256)
    dw = gradient(loss_fun, data_batch, w)
    w += - step_size * w</code>
      </pre>
    <p class="tighter">Stochastic Gradient Descent (SGD) when batch contains only 1 sample.</p>
  </div></section>

  <section class="slide"><div>
      <h2>Get your hand wet</h2>
      <p>For what does the CIFAR-10 dataset look like and data preprocessing, try notebook 00.</p>
      <p>For softmax linear classifier, try notebook 01.</p>
  </div></section>

  <section id="test-cover" class="slide cover w subheader"><div>
      <h2>Subheader<br>Example</h2>
      <img src="pics/mid.jpg" alt="">
    </div>
    <style>
    </style>
  </section>

  <section id="test-mathjax" class="slide"><div>
      <h2>Demo MathJax</h2>
      <p>\(\frac{x}{y} + \pi\)</p>
      <p>$$\frac{x}{y} + \pi$$</p>
  </div></section>

  <!-- End Slide -->
  <section id='end' class='slide cover shout w'><div>
    <h2>Thank You!</h2>
    <img src="pics/end.jpg" alt="">
  </div><style>
    #end h2 {
      position: absolute;
      text-align: left;
      top: 300px;
      left: 50px;
      color: #0376A6;
      font-size: 92px;
      opacity: 0.9;
    }
  </style></section>

  <!-- END OF SLIDE CONTENT -->
  <p class="badge"><a href="https://github.com/ccwang002/2015Talk-IPython-Parallel" target="_blank">Fork me on Github</a></p>
  <div class="progress"><div></div></div>

  <!-- Library -->
  <script src="lib/highlight/highlight.pack.js" type="text/javascript" charset="utf-8"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  <script src="lib/shower/shower.min.js"></script>
  <!-- Mathjax -->
  <!-- During local development, use localhost mathjax for speed-->
  <!--<script src="file:///Users/liang/.ipython/profile_default/static/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
  <!-- online Mathjax CDN -->
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
